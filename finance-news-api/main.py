# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18sLZwJ8zFt4g_D_wrMGoKXa36bRpdRPQ
"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import requests
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from transformers import pipeline
from nltk.stem import PorterStemmer
import requests
import pandas as pd
import json

# API parameters
VANTAGE_GET_FUNCTION = 'news_sentiment'
VANTAGE_GET_LIMIT = '1000'
VANTAGE_API_KEY = 'IT4DWFPBURJMRT3S'  # Replace with your actual API key

#Load model
task = "text-classification"
model_id = "mrm8488/deberta-v3-ft-financial-news-sentiment-analysis"
classifier = pipeline(task, model_id)

def map_sentiment(label):
    sentiment_mapping = {
        'Bearish': 'Negative',
        'Somewhat-Bearish': 'Negative',
        'Neutral': 'Neutral',
        'Bullish': 'Positive',
        'Somewhat-Bullish': 'Positive'
    }
    return sentiment_mapping.get(label, 'Unknown')


def get_top_transactions_by_authors(df, top_n=10):
    """
    Filters unique authors based on the most recent articles and displays the top_n transactions.

    Parameters:
    - df: DataFrame containing the articles.
    - top_n: The number of top transactions to display (default is 10).

    Returns:
    - A filtered DataFrame containing the top transactions by unique authors.
    """
    # Ensure the DataFrame is sorted by 'Time Published' in descending order
    sorted_df = df.sort_values(by='Time Published', ascending=False)

    # Drop articles with duplicate authors, keeping the first occurrence
    unique_authors_df = sorted_df.drop_duplicates(subset='Authors', keep='first')

    # Display the top_n entries
    top_transactions_df = unique_authors_df.head(top_n)

    return top_transactions_df


#Remove stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

lemmatizer = WordNetLemmatizer()

def lemmatize_sentence(sentence):

    tokens = nltk.word_tokenize(sentence)
    tagged = nltk.pos_tag(tokens)
    lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged]
    return ' '.join(lemmatized_sentence)

#Stemming
stemmer = PorterStemmer()
def stem_text(text):
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)


app = FastAPI()
# Allowing all middleware is optional, but good practice for dev purposes
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)
@app.get("/") # http://127.0.0.1:8000/?company_name=Google&company_ticker=GOOG&time_from=20190101&time_to=20230101
def get_stuff(company_name = "Google", company_ticker = "GOOG", time_from = "20190101", time_to = "20230101"):

    # Adjust time_from and time_to to include time (assuming start of day to end of day)
    time_from += "T0000"
    time_to += "T2359"

    # Initialize a DataFrame to store all articles
    all_articles_df = pd.DataFrame()

    # Iterate over the company tickers to make API requests

    url = f'https://www.alphavantage.co/query?function={VANTAGE_GET_FUNCTION}&tickers={company_ticker}&time_from={time_from}&time_to={time_to}&limit={VANTAGE_GET_LIMIT}&apikey={VANTAGE_API_KEY}'
    print(url)
    response = requests.get(url)
    print("Working...")
    
    if not response.status_code == 200:
        return dict(error=f"Failed to retrieve data for {company_name}: {response.status_code}")
    
    data = response.json()
    
    if 'feed' not in data:
        return dict(error=f"No articles found for {company_name}")
    
    articles = data['feed']

    # Extracting article information
    articles_info = {
        'Company': company_name,
        'Title': [article.get('title') for article in articles],
        'URL': [article.get('url') for article in articles],
        'Time Published': [article.get('time_published') for article in articles],
        'Authors': [', '.join(article.get('authors', [])) for article in articles],
        'Summary': [article.get('summary') for article in articles],
        'Source Domain': [article.get('source_domain') for article in articles],
        'Overall Sentiment Score': [article.get('overall_sentiment_score') for article in articles],
        'Overall Sentiment Label': [article.get('overall_sentiment_label') for article in articles],
    }

    # Convert to DataFrame
    df = pd.DataFrame(articles_info)

    # Apply the sentiment mapping
    df['Mapped Sentiment'] = df['Overall Sentiment Label'].apply(map_sentiment)

    # Append to the all_articles_df DataFrame
    all_articles_df = pd.concat([all_articles_df, df], ignore_index=True)
    
    # Assuming all_articles_df is the DataFrame with all the articles data
    top_transactions_df = get_top_transactions_by_authors(all_articles_df, 100)
    top_transactions_df[['Company', 'Authors', 'Time Published', 'Title', 'Summary', 'Mapped Sentiment', 'Overall Sentiment Score']]

    # Create a temporary df that concatenates the "Title" and "Summary" for pre-processing
    top_transactions_df['Title & Summary'] = top_transactions_df['Title'] + " " + top_transactions_df['Summary']

    # Now create a new DataFrame with the 'Title & Summary' column for further processing
    text_df = pd.DataFrame(top_transactions_df[['Company','Authors','Time Published','Title & Summary',]])

    # Apply preprocessing functions and pass it to the predict model to get a sentiment label and score
    # Ensure you are using the correct DataFrame here, which is `text_df` and not `df`
    text_df['Preprocessed'] = text_df['Title & Summary'].apply(remove_stopwords).apply(lemmatize_sentence).apply(stem_text)

    # Apply the model to the 'Preprocessed' column
    # Create new columns for sentiment and score
    text_df['Model Sentiment'] = text_df['Preprocessed'].apply(lambda x: classify_sentiment(x)['label'])
    text_df['Model Sentiment Score'] = text_df['Preprocessed'].apply(lambda x: classify_sentiment(x)['score'])

    # Now you have a DataFrame with the sentiment and score
    new_df = text_df[['Company','Authors','Time Published','Title & Summary','Preprocessed', 'Model Sentiment', 'Model Sentiment Score']]

    print("Done.")

    return json.loads(new_df.to_json())

    #return dict(new_df=str(new_df.count()))
    

@app.get("/ping") # Healthcheck endpoint
def ping():
    return dict(ping="ok")


@app.get("/dummy") # endpoint for returning dummy data
def dummy():
    return {
        "Company":{
        "0":"Google",
        "1":"Google"
        },
        "Authors":{
        "0":"",
        "1":"Ashley Capoot"
        },
        "Time Published":{
        "0":"20230101T214500",
        "1":"20230101T213259"
        },
        "Title & Summary":{
        "0":"HZL forays into fertiliser production, to set up 0.5 MT plant in Rajasthan | The Financial Express HZL forays into fertiliser production, to set up 0.5 MT plant in Rajasthan The Financial Express ...",
        "1":"More social media regulation is coming in 2023, members of Congress say Legislators and advocates say they are looking to further regulate social media companies in 2023."
        },
        "Preprocessed":{
        "0":"hzl foray fertilis product , set 0.5 mt plant rajasthan | the financi express hzl foray fertilis product , set 0.5 mt plant rajasthan the financi express ...",
        "1":"more social medium regul come 2023 , member congress say legisl advoc say look regul social medium compani 2023 ."
        },
        "Model Sentiment":{
        "0":"neutral",
        "1":"neutral"
        },
        "Model Sentiment Score":{
        "0":0.9994545579,
        "1":0.999353826
        }
    }

@app.get("/predict") #Sentiment for each article
def predict(text):
    text = classify_sentiment(text)
    return text

# Function to classify sentiment of a given text
def classify_sentiment(text):
    return classifier(text)[0]
